ğŸ“Š Communication Analysis Tool for Human-AI Interaction Driving Simulator Experiments
This project is part of a screening test to analyze communication in a simulated environment. It consists of two main Python programs:

Data Handling & Analysis â€“ Transcribes and analyzes sentiment from driving simulator video files.
Data Understanding & Manipulation â€“ Visualizes transcription frequency and sentiment results using plots.

ğŸ§ª 1. Data Handling & Analysis
This part handles:

Importing and preprocessing video/audio files
Performing speech-to-text transcription (using Whisper)
Segmenting audio into 5-second chunks
Mapping each transcription to its start timestamp
Sentiment analysis using a pre-trained transformer model (e.g., distilbert-base-uncased-finetuned-sst-2-english)
Saving results as a CSV with:
start_time
end_time
transcription
sentiment
â¡ï¸ Output is saved as: output/transcription_<video_name>.csv

ğŸ“ˆ 2. Data Understanding & Visualization
This part:

Loads a generated transcription CSV
Generates:
ğŸ“Š Histogram of transcribed line frequency per 5-second bucket
ğŸ“ˆ Sentiment distribution plot (positive / neutral / negative)
Both plots are automatically generated and displayed using matplotlib and seaborn.

â–¶ï¸ How to Run
âœ… Option 1: Run Online
You can run this project directly in:
âœ… Kaggle Notebook â€“ All dependencies are pre-installed update the dataset path.
âœ… Google Colab â€“ Upload the .ipynb file and mount your dataset.

No setup required in either platform. Just upload the dataset and run the notebook cells.
ğŸ’» Option 2: Run Locally
Clone this repository and download the dataset from (https://alabama.app.box.com/s/vnwa6b4qi42ts39exv8jej6vzabdu78k)

Install dependencies:
pip install -r requirements.txt
Update the path to your video file in the notebook code (ensure it matches your local directory structure)

âœ… Testing
Tested on Kaggle using the sample video: Experimenter_CREW_999_1_All_1731617801.mp4
Transcription and sentiment analysis were verified manually.
Visual plots confirmed accurate timestamp bucketing and sentiment counts.

ğŸ“Œ Notes
Audio segmentation ensures each chunk â‰¤ 5 seconds
Sentiment is derived from transcribed text, not tone
No external/proprietary API is used; only open-source models
